{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acb4f9f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting inflect\n",
      "  Downloading inflect-5.6.0-py3-none-any.whl (33 kB)\n",
      "Installing collected packages: inflect\n",
      "Successfully installed inflect-5.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install inflect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31628744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Downloading textblob-0.17.1-py2.py3-none-any.whl (636 kB)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\safae\\anaconda3\\lib\\site-packages (from textblob) (3.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\safae\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (1.1.0)\n",
      "Requirement already satisfied: click in c:\\users\\safae\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (8.0.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\safae\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (4.64.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\safae\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (2022.3.15)\n",
      "Requirement already satisfied: colorama in c:\\users\\safae\\anaconda3\\lib\\site-packages (from click->nltk>=3.1->textblob) (0.4.4)\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.17.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "227266da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting elasticsearch==7.0.0\n",
      "  Downloading elasticsearch-7.0.0-py2.py3-none-any.whl (80 kB)\n",
      "Requirement already satisfied: urllib3>=1.21.1 in c:\\users\\safae\\anaconda3\\lib\\site-packages (from elasticsearch==7.0.0) (1.26.9)\n",
      "Installing collected packages: elasticsearch\n",
      "Successfully installed elasticsearch-7.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install elasticsearch==7.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62f5fda7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kafka-pythonNote: you may need to restart the kernel to use updated packages.\n",
      "  Using cached kafka_python-2.0.2-py2.py3-none-any.whl (246 kB)\n",
      "Installing collected packages: kafka-python\n",
      "Successfully installed kafka-python-2.0.2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install kafka-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "af865b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tweepy\n",
      "  Using cached tweepy-4.10.0-py3-none-any.whl (94 kB)\n",
      "Requirement already satisfied: requests<3,>=2.27.0 in c:\\users\\safae\\anaconda3\\lib\\site-packages (from tweepy) (2.27.1)\n",
      "Collecting requests-oauthlib<2,>=1.2.0\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting oauthlib<4,>=3.2.0\n",
      "  Using cached oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\safae\\anaconda3\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\safae\\anaconda3\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\safae\\anaconda3\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\safae\\anaconda3\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (2.0.4)\n",
      "Installing collected packages: oauthlib, requests-oauthlib, tweepy\n",
      "Successfully installed oauthlib-3.2.0 requests-oauthlib-1.3.1 tweepy-4.10.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e4be669c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from kafka import KafkaProducer\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "84043c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"API ACCESS KEYS\"\"\"\n",
    "\n",
    "consumerKey =\"dd4dHU4rIAL7LBYFu43q86BRA\"\n",
    "consumerSecret =\"6ttQ2rdw9v6VjfPOq0OAp3pY7gcKu0Lw7mFV86rsPfK1iqNLjo\"\n",
    "accessToken =\"1322415417638461440-7LsHymm2rcAZwRb7h7yfPjJXqJwd22\"\n",
    "accessTokenSecret =\"msUlh9xwDBU9TkM1xsLD2UVtnYsVfPKugjiLy7lfTyQon\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d2032d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "producer = KafkaProducer(bootstrap_servers='localhost:9092')\n",
    "search_term = 'ukraine'\n",
    "topic_name = 'test'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9168319d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622ec7f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "eed779c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\safae\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\safae\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\safae\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import inflect\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords # used for preprocessing\n",
    "from nltk.stem import WordNetLemmatizer # used for preprocessing\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "from operator import add\n",
    "from kafka import KafkaConsumer\n",
    "import json\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a631a57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "# create instance of elasticsearch\n",
    "es = Elasticsearch([{'host':'localhost','port':9200}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a47c6e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.ping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9c0c2d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:kafka.coordinator.consumer:group_id is None: disabling auto-commit.\n"
     ]
    }
   ],
   "source": [
    "topic_name = 'teste'\n",
    "# ------------------- your consummer ------------------- -----\n",
    "consumer = KafkaConsumer(\n",
    "    topic_name,\n",
    "     bootstrap_servers=['localhost:9092'],\n",
    "     auto_offset_reset='latest',\n",
    "     enable_auto_commit=True,\n",
    "     auto_commit_interval_ms =  5000,\n",
    "     fetch_max_bytes = 128,\n",
    "     max_poll_records = 100,\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fde3a641",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------Traitement du data -----------------------\n",
    "\n",
    "def remove_urls(text):\n",
    "    new_text = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^A-Za-z# \\t])|(\\w+:\\/\\/\\S+)\",\" \",text).split())\n",
    "    return new_text\n",
    "\n",
    "# make all text lowercase\n",
    "def text_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "# make all text Uppercase\n",
    "\n",
    "def text_uppercase(text):\n",
    "    return text.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4c8ec8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------delete numbers from text ---------\n",
    "\n",
    "def remove_numbers(text):\n",
    "    result = re.sub(r'\\d+', '', text)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3104d29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------- delete ponctuation existent on text ----------\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "\n",
    "# ----------------convert list to words ---------------\n",
    "def tokenize(text):\n",
    "    text = word_tokenize(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "04cdb1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------- delete words non significant ---------\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def remove_stopwords(text):\n",
    "    text = [i for i in text if not i in stop_words]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dbd81647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------get the origins of words--------------\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize(text):\n",
    "    text = [lemmatizer.lemmatize(token) for token in text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2a84359f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------remplace all number with word notation-----------\n",
    "def numbers_to_char(text):\n",
    "    wordtoken = nltk.word_tokenize(text)\n",
    "    p = inflect.engine()\n",
    "    new_Text = []\n",
    "    for word in wordtoken:\n",
    "        if word.isdigit():\n",
    "            newword = p.number_to_words(word)\n",
    "            new_Text.append(newword)\n",
    "        else:\n",
    "            new_Text.append(word)\n",
    "    return new_Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "eee8d948",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------end analyse feedback using textBlob--------------------------------\n",
    "def analyze_sentiment(text):\n",
    "    testimonial = TextBlob(text)\n",
    "    return testimonial.sentiment.polarity\n",
    "def get_sentiment_tuple(sent):\n",
    "    neutral_threshold = 0.05\n",
    "    if sent >= neutral_threshold:       # positive\n",
    "        return (0, 0, 1),\"positive\"\n",
    "    elif sent > -neutral_threshold:     # neutral\n",
    "        return (0, 1, 0),\"neutral\"\n",
    "    else:                               # negative\n",
    "        return (1, 0, 0),\"negative\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fa5cbbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter=0 \n",
    "def topHashtag(text):\n",
    "    return text.split(\" \")\n",
    "def getNouns(txt):\n",
    "    blob = TextBlob(txt)\n",
    "    print(blob.noun_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d992812e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the producer produce tweets\n"
     ]
    }
   ],
   "source": [
    "class TweetListener(tweepy.Stream):\n",
    "\n",
    "    def on_data(self, raw_data):\n",
    "        logging.info(raw_data)\n",
    "        producer.send(topic_name, value=raw_data)\n",
    "        return True\n",
    "\n",
    "    def on_error(self, status_code):\n",
    "        if status_code == 420:\n",
    "            # returning False in on_data disconnects the stream\n",
    "            return False\n",
    "\n",
    "    def start_streaming_tweets(self, search_term):\n",
    "        self.filter(track=search_term, stall_warnings=True, languages=[\"en\"])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"the producer produce tweets\")\n",
    "    twitter_stream = TweetListener(consumerKey, consumerSecret, accessToken, accessTokenSecret)\n",
    "    twitter_stream.start_streaming_tweets(search_term)\n",
    "\n",
    "\n",
    "    for msg in consumer:\n",
    "            #print(str(1));\n",
    "            time.sleep(2)\n",
    "\n",
    "            data= text_lowercase(msg.value)\n",
    "\n",
    "\n",
    "            #------hashtages----------------------------------------------\n",
    "            dict_data=json.loads(data)\n",
    "            key=\"text\"\n",
    "            if (key in dict_data.keys()):\n",
    "                print(dict_data[\"text\"])\n",
    "                hashtags = re.findall(\"#[a-zA-Z0-9_]{1,200}\", dict_data[\"text\"])\n",
    "                print(\"hashtags :\",hashtags)\n",
    "\n",
    "            #---feedback-----------------------------------\n",
    "                tweets=TextBlob(dict_data[\"text\"])\n",
    "                sentiments=analyze_sentiment(dict_data[\"text\"])\n",
    "\n",
    "            #----------best and bad tweets ------------------\n",
    "                frequencetweets= text_lowercase(dict_data[\"text\"])\n",
    "                frequencetweets=remove_urls(frequencetweets)\n",
    "\n",
    "            #----------- frequency of words---------------\n",
    "                filtreData= text_lowercase(dict_data[\"text\"])\n",
    "                filtredData=remove_urls(filtreData)\n",
    "                filtredData=remove_punctuation(filtredData)\n",
    "                filtredData=tokenize(filtredData)\n",
    "                filtredData=remove_stopwords(filtredData)\n",
    "                FrequencesMots=lemmatize(filtredData)\n",
    "\n",
    "                print(FrequencesMots)\n",
    "                analysentimens,feedback=get_sentiment_tuple(sentiments)\n",
    "                print(\"val :\",analysentimens,\"  sentiments : \",feedback)\n",
    "\n",
    "            # ------------------- send all result to elastic search --------------------- \n",
    "\n",
    "                es.index(index=\"tweet_index\" ,\n",
    "                        doc_type=\"test_doc\",\n",
    "                        body={\n",
    "                                \"author\": dict_data[\"user\"][\"screen_name\"],\n",
    "                                \"date\": dict_data[\"created_at\"],\n",
    "                                \"message\": dict_data[\"text\"], \n",
    "                                \"sentimentsPer\":sentiments,\n",
    "                                \"feedback\":feedback,\n",
    "                                \"hashtag\":hashtags,\n",
    "                                \"FrequencMots\":FrequencesMots,\n",
    "                                \"FrequenceTweets\":frequencetweets\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "\n",
    "            #-------------------------- optional ---------------------\n",
    "            #time.sleep(5)\n",
    "            #print(str(tweets))\n",
    "            print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "865c13eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\safae\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\omw-1.4.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " #nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f408a73a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
